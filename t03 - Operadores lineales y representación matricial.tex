\portada

\begin{esquemaExplorador}
  \temaEsquema{Transformaciones lineales}{
    \conceptoEsquema{Definición y propiedades}{}
    \conceptoEsquema{Núcleo e imagen}{}
    \conceptoEsquema{Inyectividad y sobreyectividad}{}
  }
  \temaEsquema{Operadores lineales}{
    \conceptoEsquema{Operadores como endomorfismos}{}
    \conceptoEsquema{Composición de operadores}{}
    \conceptoEsquema{Operador inverso}{}
  }
  \temaEsquema{Representación matricial}{
    \conceptoEsquema{Matriz asociada a una transformación}{}
    \conceptoEsquema{Cambio de base}{}
    \conceptoEsquema{Isomorfismo con matrices}{}
  }
  \temaEsquema{Matrices complejas}{
    \conceptoEsquema{Operaciones matriciales}{}
    \conceptoEsquema{Matrices especiales}{}
  }
\end{esquemaExplorador}

\unirsection{Ideas clave}

\subsection{Introducción y objetivos}

Los operadores lineales constituyen el puente conceptual entre la estructura algebraica abstracta de los espacios vectoriales y su representación computacional mediante matrices. En computación cuántica, este concepto es fundamental ya que todas las operaciones que pueden realizarse sobre sistemas cuánticos se describen mediante operadores lineales unitarios.

La importancia de los operadores lineales en computación cuántica se refleja en múltiples aspectos:

\begin{itemize}
  \item Las \textbf{puertas cuánticas} son operadores unitarios que actúan sobre qubits.
  \item La \textbf{evolución temporal} de sistemas cuánticos se describe mediante operadores unitarios.
  \item Los \textbf{observables cuánticos} son operadores hermitianos.
  \item Los \textbf{algoritmos cuánticos} se construyen como secuencias de operadores lineales.
\end{itemize}

En este tema desarrollaremos la teoría de transformaciones lineales entre espacios vectoriales complejos, con especial énfasis en los operadores (transformaciones de un espacio en sí mismo) y su representación matricial. Esta conexión entre conceptos abstractos y representaciones concretas es esencial para la implementación práctica de algoritmos cuánticos.

\subsection{Transformaciones lineales}

\begin{defi}[Transformación lineal]
  Sean $V$ y $W$ espacios vectoriales. Una función $T: V \to W$ es una \textbf{transformación lineal} o \textbf{homomorfismo}, si para todo $u, v \in V$ y $\alpha, \beta \in \C$ se tiene que
  $$T(\alpha u + \beta v) = \alpha T(u) + \beta T(v)\,.$$
\end{defi}
Una definición equivalente es dar las propiedades por separado.

\begin{prop}
  Una transformación entre espacios vectoriales, $T$ es lineal si y solo si
  \begin{enumerate}
    \item \textbf{Conservación de la suma}. Para todo $u, v \in V$, se tiene que $T(u + v) = T(u) + T(v)$.
    \item \textbf{Conservación del producto por escalar}. Para todo $v \in V$ y $\alpha \in \C$, se tiene que $T(\alpha v) = \alpha T(v)$.
  \end{enumerate}
\end{prop}

\begin{eje}[Transformaciones lineales básicas]
  \begin{enumerate}
    \item \textbf{Transformación identidad}: $I: V \to V$ definida por $I(v) = v$.

    \item \textbf{Transformación cero}: $O: V \to W$ definida por $O(v) = 0$.

    \item \textbf{Escalamiento}: Dado $\alpha \in \C$, $S_\alpha: \C^n \to \C^n$ definida por $S_\alpha(v) = \alpha v$.

    \item \textbf{Proyección}: $P: \C^3 \to \C^2$ definida por $P(x, y, z) = (x, y)$.

    \item \textbf{Rotación en $\C^2$}: Dado $\theta \in \R^+$, $R_\theta: \C^2 \to \C^2$ definida por
          $$R_\theta(x, y) = \begin{pmatrix} x\cos\theta - y\sin\theta \\ x\sin\theta + y\cos\theta \end{pmatrix}\,.$$
  \end{enumerate}
\end{eje}

\begin{prop}
  \label{prop:propiedadesTL}
  Si $T: V \to W$ es una transformación lineal, entonces:
  \begin{enumerate}
    \item $T(0) = 0$.
    \item $T(-v) = -T(v)$ para todo $v \in V$.
    \item $T$ está completamente determinada por su acción sobre cualquier base de $V$. Es decir, si existe $\mathcal{B} = \{v_1, \ldots, v_n\}$ una base de $V$ y $U:V\to W$ otra transformación lineal tal que $U(v_i) = T(v_i)$ para todo $i$, entonces $U = T$.
  \end{enumerate}
\end{prop}

\begin{defi}
  Sea $T: V \to W$ una transformación lineal. Diremos que $T$ es:
  \begin{itemize}
    \item \textbf{Monomorfismo} si es inyectiva.
    \item \textbf{Epimorfismo} si es sobreyectiva.
    \item \textbf{Isomorfismo} si es biyectiva. En este caso también se dice que los espacios vectoriales son isomorfos y se escribe como $V \cong W$.
  \end{itemize}
\end{defi}

\begin{defi}[Núcleo]
  Sea $T: V \to W$ una transformación lineal. Llamamos \textbf{núcleo (o kernel)} de $T$ a
  \[
    \Ker(T) = \{v \in V : T(v) = 0\}
  \]
\end{defi}

\begin{prop}
  Sea $T: V \to W$ una transformación lineal. Entonces:
  \begin{enumerate}
    \item $\Ker(T)$ es un subespacio de $V$.
    \item $\Ima(T)$ es un subespacio de $W$.
    \item $T$ es inyectiva si y solo si $\Ker(T) = \{0\}$.
  \end{enumerate}
\end{prop}

\begin{eje}[Cálculo de núcleo e imagen]
  Considerar la transformación $T: \C^3 \to \C^2$ definida por:
  $$T(x, y, z) = (x + iy, 2x - z)$$
  \textbf{Núcleo:} Para obtener el núcleo de la transformación lineal, tenemos que resolver la ecuación obtenida al plantear la igualdad $T(v) = 0$.
  \begin{align}
    x + iy & = 0 \\
    2x - z & = 0
  \end{align}
  De la primera ecuación: $x = -iy$. De la segunda: $z = 2x = -2iy$. Por tanto
  $$
    \Ker(T) = \{(-iy, y, -2iy)\mid y \in \C\} = \text{gen}\{(-i, 1, -2i)\}
  $$

  \textbf{Imagen:} Como $T$ es lineal, $\Ima(T) = \text{gen}\{T(e_1), T(e_2), T(e_3)\}$, donde $\{e_1, e_2, e_3\}$ es la base canónica de $\C^3$.
  \begin{align}
    T(e_1) & = T(1, 0, 0) = (1, 2), \\
    T(e_2) & = T(0, 1, 0) = (i, 0), \\
    T(e_3) & = T(0, 0, 1) = (0, -1)
  \end{align}
  Como $(1, 2) = -i(i, 0) - 2(0, -1)$ y los vectores $(i, 0)$ y $(0, -1)$ son linealmente independientes en $\C^2$, $\Ima(T) = \text{gen}\{(i, 0), (0, -1)\} = \C^2$.
\end{eje}

\begin{theo}
  \label{th:descomposicion}
  Sea $T: V \to W$ una transformación lineal. Entonces:
  \begin{itemize}
    \item Existe $V^\prime \subseteq V$ tal que $V = \Ker(T) \oplus V^\prime$.
    \item Existe $W^\prime \subseteq W$ tal que $W = \Ima(T) \oplus W^\prime$.
    \item $V/\Ker(T) \cong \Ima(T)$.
  \end{itemize}
\end{theo}

Como consecuencia directa del Teorema~\ref{th:descomposicion} obtenemos el siguiente resultado fundamental.

\begin{theo}
  \label{th:dimension}
  Sea $T: V \to W$ una transformación lineal. Entonces
  \[
    \dim(V) = \dim(\Ker(T)) + \dim(\Ima(T))\,.
  \]
\end{theo}

\subsection{Operadores lineales}

\begin{defi}[Operador lineal]
  Un \textbf{operador lineal} o \textbf{endomorfismo} sobre un espacio vectorial $V$ es una transformación lineal $T: V \to V$. El conjunto de todos los operadores lineales de $V$ se denota $\mathcal{L}(V)$.
\end{defi}

Los operadores lineales tienen propiedades especiales debido a que el espacio de salida coincide con el de entrada, lo que permite usar conceptos como la composición o la inversa.

\begin{prop}
  Sean $R,S$ y $T$ operadores lineales. La composición de operadores lineales satisface:
  \begin{enumerate}
    \item \textbf{Asociatividad:} $(R \comp S) \comp T = R \comp (S \comp T)$.
    \item \textbf{Elemento neutro:} $I \comp T = T \comp I = T$.
    \item \textbf{Distributividad:} $R \comp (S + T) = R \comp S + R \comp T$.
  \end{enumerate}
\end{prop}

\begin{theo}[Caracterización de la invertibilidad]
  Un operador lineal $T \in \mathcal{L}(V)$ es invertible si y solo si es un isomorfismo. Equivalentemente:
  \begin{enumerate}
    \item $T$ es inyectivo ($\Ker(T) = \{0\}$).
    \item $T$ es sobreyectivo ($\Ima(T) = V$).
  \end{enumerate}
\end{theo}

Aplicando el Teorema~\ref{th:descomposicion} a operadores lineales obtenemos la siguiente consecuencia.
\begin{theo}
  Sea $T \in \mathcal{L}(V)$ un operador lineal, se cumple
  \[
    V = \Ker(T) \oplus \Ima(T)\,.
  \]
\end{theo}

\subsection{Representación matricial de transformaciones lineales}

La representación matricial establece una correspondencia biyectiva entre transformaciones lineales y matrices, permitiendo cálculos computacionales eficientes.

\begin{defi}[Matriz de una transformación lineal]
  Sea $T: V \to W$ una transformación lineal. Sean $\mathcal{B}_V = \{v_1, \ldots, v_n\}$ y $\mathcal{B}_W = \{w_1, \ldots, w_m\}$ bases de $V$ y $W$ respectivamente.

  Para cada $j = 1, \ldots, n$, expresamos $T(v_j)$ en la base $\mathcal{B}_W$
  \[
    T(v_j) = \sum_{k=1}^m a_{jk} w_k\,.
  \]

  La \textbf{matriz de} $T$ \textbf{respecto a las bases} $\mathcal{B}_V$ \textbf{y} $\mathcal{B}_W$ es
  \[
    [T]_{\mathcal{B}_V}^{\mathcal{B}_W} = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1m} \\ a_{21} & a_{22} & \cdots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nm} \end{pmatrix}\,.
  \]
\end{defi}

En el caso de considerarse $\mathcal{B}_V$ y $\mathcal{B}_W$ las bases canónicas, la matriz de la transformación se denota simplemente como $[T]$.

\begin{nota}
  La $j$-ésima fila de $[T]_{\mathcal{B}_V}^{\mathcal{B}_W}$ contiene las coordenadas de $T(v_j)$ respecto a la base $\mathcal{B}_W$
  \setlength{\belowdisplayskip}{0pt}
  \begin{equation*}
    T(v_j)_{\mathcal{B}_W} = (v_j)_{\mathcal{B}_V} [T]_{\mathcal{B}_V}^{\mathcal{B}_W}\,.
  \end{equation*}
\end{nota}

\begin{eje}[Matriz de una transformación]
  Sea $T: \C^2 \to \C^3$ definida por $T(x, y) = (x + iy, 2x, x - y)$.

  Usando las bases canónicas:
  \begin{align*}
    T(e_1) & = T(1, 0) = (1, 2, 1)     \\
    T(e_2) & = T(0, 1) = (i, 0, -1)\,.
  \end{align*}
  Por tanto
  \begin{equation*}
    [T] = \begin{pmatrix} 1 & 2 & 1 \\ i & 0 & -1 \end{pmatrix}
  \end{equation*}
\end{eje}

\begin{theo}[Cambio de base]
  \label{th:cambio_base}
  Sea $T\in\mathcal{L}(V)$ un operador lineal y $\mathcal{B}_1$, $\mathcal{B}_2$, $\mathcal{B}_3$ y $\mathcal{B}_4$ bases de $V$. Existe una matriz $P$ invertible tal que las matrices $[T]_{\mathcal{B}_1}^{\mathcal{B}_3}$ y $[T]_{\mathcal{B}_2}^{\mathcal{B}_4}$ están relacionadas por
  $$[T]_{\mathcal{B}_2}^{\mathcal{B}_4} = P[T]_{\mathcal{B}_1}^{\mathcal{B}_3}P^{-1}\,.$$
  Llamamos a $P$ la matriz de \textbf{cambio de base}.
\end{theo}

El teorema de cambio de base~\ref{th:cambio_base} es muy importante, pues nos permite verificar muchas propiedades sobre operadores lineales, con solo comprobarlo con la matriz asociada a las bases canónicas.

\subsection{Matrices complejas como operadores}

Hemos visto, que todo operador lineal $T\in\mathcal{L}(V)$ puede representarse como una matriz cuadrada $A\in\C^{n\times n}$, donde $n=\dim(V)$. Pero el recíproco es claramente cierto, es decir, todo matriz cuadrada define un operador lineal.

Cada matriz compleja $A \in \mathcal{M}_n(\C)$ define naturalmente un operador lineal $T_A: \C^n \to \C^n$ mediante $T_A(v) = vA$. Esta correspondencia permite estudiar propiedades algebraicas de las matrices a través de la teoría de operadores, y viceversa.

\begin{defi}
  Sea $A\in \mathcal{M}_n(\C)$. Llamamos \textbf{matriz adjunta} a la matriz $A^\dagger$ definida por
  \[
    (A^\dagger)_{ij} = \conj{A_{ji}}\,.
  \]
\end{defi}

\begin{defi}
  Sea $A \in \mathcal{M}_n(\C)$. Decimos que $A$ es:
  \begin{itemize}
    \item
          \textbf{Hermítica}: si $A^\dagger = A$.
    \item
          \textbf{Unitaria}: si $A^\dagger A = AA^\dagger = I$.
    \item
          \textbf{Normal}: si $A^\dagger A = AA^\dagger$.
  \end{itemize}

\end{defi}

Estudiaremos con más detalle estas matrices en temas posteriores, ya que son fundamentales en computación cuántica.

\begin{eje}[Matrices de Pauli]
  Las matrices de Pauli son matrices hermitianas y unitarias fundamentales en computación cuántica:
  \begin{align}
    \sigma_x = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}, \quad
    \sigma_y = \begin{pmatrix} 0 & -i \\ i & 0 \end{pmatrix}, \quad
    \sigma_z = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}
  \end{align}

  Estas matrices satisfacen:
  \begin{itemize}
    \item $\sigma_i^2 = I$ para $i = x, y, z$.
    \item $\sigma_x\sigma_y = i\sigma_z$, $\sigma_y\sigma_z = i\sigma_x$, $\sigma_z\sigma_x = i\sigma_y$.
    \item $\{\sigma_x, \sigma_y\} = \sigma_x\sigma_y + \sigma_y\sigma_x = 0$ (anticonmutan).
    \item Junto con la identidad $I$, forman una base de $\C^{2 \times 2}$.
  \end{itemize}
\end{eje}

\begin{eje}[Matriz de Hadamard]
  La matriz de Hadamard es otra de las matrices unitarias fundamentales
  \[
    H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}
  \]
  Se verifica que $H^2 = I$ y transforma la base canónica en:
  \begin{align*}
    H(1, 0) & = \frac{1}{\sqrt{2}}(1, 1)\,.  \\
    H(0, 1) & = \frac{1}{\sqrt{2}}(1, -1)\,.
  \end{align*}
\end{eje}

El motivo de dar a estas propiedades los mismos nombres que las de los operadores lineales es que son equivalentes.

\begin{prop}
  \begin{itemize}
    \item Sea $A \in \mathcal{M}_n(\C)$. Entonces $A$ es hermitiana, unitaria o normal si y solo si $T_A$ es hermitiano, unitario o normal, respectivamente.
    \item Sea $T \in \mathcal{L}(V)$. Entonces $T$ es hermitiano, unitario o normal si y solo si $[T]$ es hermitiana, unitaria o normal, respectivamente.
  \end{itemize}
\end{prop}

Observar que las propiedades de hermitianidad, unitariedad y normalidad no dependen de las bases escogidas.

\subsection{Valores y vectores propios}

\begin{defi}[Valor y vector propio]
  Sea $A \in \mathcal{M}_n(\C)$. Un escalar $\lambda \in \C$ es un \textbf{valor propio} de $A$ si existe un vector no nulo $v \in \C^n$ tal que:
  $vA = \lambda v$.

  El vector $v$ se llama \textbf{vector propio} asociado al valor propio $\lambda$.
\end{defi}

Para obtener los valores propios de una matriz $A$, se resuelve el sistema $(A - \lambda I)v = 0$. Este sistema tiene soluciones no triviales si y solo si $\det(A - \lambda I) = 0$.

\begin{defi}[Polinomio característico]
  El \textbf{polinomio característico} de $A \in \mathcal{M}_n(\C)$ es
  \[
    p_A(\lambda) = \det(A - \lambda I)\,.
  \]
\end{defi}

\begin{prop}
  Los valores propios de $A\in\mathcal{M}_n(\C)$ son las raíces del polinomio característico $p_A(\lambda)$.
\end{prop}

\begin{eje}
  Consideremos la matriz de Pauli $\sigma_z = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$, cuyo polinomio característico es

  \[
    p_{\sigma_z}(\lambda) = \det\begin{pmatrix} 1-\lambda & 0 \\ 0 & -1-\lambda \end{pmatrix} = (1-\lambda)(-1-\lambda) = \lambda^2 - 1\,.
  \]

  Los valores propios son $\lambda_1 = 1$ y $\lambda_2 = -1$.

  Ahora calculemos los vectores propios, primero para $\lambda_1 = 1$
  \[
    v(\sigma_z - I) = (v_1, v_2)\begin{pmatrix} 0 & 0 \\ 0 & -2 \end{pmatrix} = 0 \Rightarrow v_2 = 0\,.
  \]
  Por tanto, $(v_1, 0)$ con $v_1 \neq 0$ es un vector propio de $\sigma_z$ asociado al valor propio $\lambda_1 = 1$.

  Los vectores propios para $\lambda_2 = -1$ son
  \[
    v(\sigma_z + I) = (v_1, v_2)\begin{pmatrix} 2 & 0 \\ 0 & 0 \end{pmatrix} = 0 \Rightarrow v_1 = 0\,.
  \]
  Por tanto, $(0, v_2)$ con $v_2 \neq 0$ es un vector propio de $\sigma_z$ asociado al valor propio $\lambda_2 = -1$.

  Tenemos que $(v_1, 0)$ y $(0, v_2)$ son vectores propios de $\sigma_z$ asociados a los valores propios $\lambda_1 = 1$ y $\lambda_2 = -1$, respectivamente, que forman además una base ortonormal de $\C^2$.
\end{eje}

\begin{theo}[Diagonalización]
  \label{th:diagonalizacion}
  Una matriz $A \in \mathcal{M}_n(\C)$ es diagonalizable si y solo si tiene $n$ vectores propios linealmente independientes. En tal caso, existe una matriz invertible $P$ tal que:
  \[
    A = PDP^{-1}\,,
  \]
  donde $D$ es diagonal con los valores propios de $A$ en la diagonal y $P$ es la matriz cuyas columnas son los vectores propios de $A$.
\end{theo}

\begin{eje}
  Consideremos la matriz de Hadamard $H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$.
  Si calculamos su polinomio característico tenemos
  \[
    p_H(\lambda) = \det(H - \lambda I) = \begin{vmatrix} \frac{1}{\sqrt{2}}-\lambda & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}-\lambda \end{vmatrix} = (\frac{1}{\sqrt{2}}-\lambda)(-\frac{1}{\sqrt{2}}-\lambda) - \frac{1}{2} =  \lambda^2 - 1\,.
  \]

  Los valores propios son $\lambda_1 = 1$ y $\lambda_2 = -1$.

  Para $\lambda_1 = 1$ tenemos
  \[
    v(H - I) = (v_1, v_2)\begin{pmatrix} \frac{1-\sqrt{2}}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{-1-\sqrt{2}}{\sqrt{2}} \end{pmatrix} = 0 \Rightarrow \begin{cases}
      v_1(1-\sqrt{2}) + v_2 = 0 \\
      v_1 - v_2(1+\sqrt{2}) = 0\,.
    \end{cases}
  \]
  Por tanto, $((1+\sqrt{2})v_2, v_2)$ con $v_2 \neq 0$ es un vector propio de $H$ asociado al valor propio $\lambda_1 = 1$.

  Para $\lambda_2 = -1$ tenemos
  \[
    v(H + I) = (v_1, v_2)\begin{pmatrix} \frac{1+\sqrt{2}}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{-1+\sqrt{2}}{\sqrt{2}} \end{pmatrix} = 0 \Rightarrow \begin{cases}
      v_1(1+\sqrt{2}) + v_2 = 0 \\
      v_1 + v_2(-1+\sqrt{2}) = 0\,.
    \end{cases}
  \]
  Por tanto, $((1-\sqrt{2})v_2, v_2)$ con $v_2 \neq 0$ es un vector propio de $H$ asociado al valor propio $\lambda_2 = -1$.

  Tenemos que $\frac{1}{2}(1+\sqrt{2}, 1)$ y $\frac{1}{2}(1-\sqrt{2}, 1)$ son vectores propios ortonormales de $H$ asociados a los valores propios $\lambda_1 = 1$ y $\lambda_2 = -1$, respectivamente.

  Por el teorema de la diagonalización tenemos que
  \[
    H   = \frac{1}{2}\begin{pmatrix} 1+\sqrt{2} & 1-\sqrt{2} \\ 1 & 1 \end{pmatrix}\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}2\begin{pmatrix} \frac{1}{2\sqrt{2}} & \frac{-1-\sqrt{2}}{2\sqrt{2}} \\ \frac{-1}{2\sqrt{2}} & \frac{1+\sqrt{2}}{2\sqrt{2}} \end{pmatrix}\,.
  \]
\end{eje}

\subsection{Espacios de operadores}

El estudio de los operadores lineales no se limita a sus propiedades individuales. El conjunto de todos los operadores lineales definidos sobre un espacio vectorial forma, a su vez, un nuevo espacio vectorial. Esta estructura permite sumar operadores y multiplicarlos por escalares, extendiendo las herramientas del álgebra lineal al propio conjunto de transformaciones. En el contexto de la computación cuántica, esta perspectiva es esencial para comprender la estructura del espacio de observables y las operaciones permitidas sobre ellos.

\begin{prop}
  El conjunto $\mathcal{L}(V)$ de todos los operadores lineales en un espacio vectorial $V$ forma un espacio vectorial con las operaciones:
  \begin{itemize}
    \item $(S + T)(v) = S(v) + T(v)$
    \item $(\alpha T)(v) = \alpha T(v)$
  \end{itemize}
\end{prop}

\begin{theo}[Correspondencia entre operadores lineales y matrices]
  La representación matricial establece un isomorfismo de espacios vectoriales
  $$\mathcal{L}(V) \cong \C^{n^2}\cong \mathcal{M}_n(\C)\,,$$
  donde $\dim(V) = n$.

  Además, si $S, T\in\mathcal{L}(V)$ son operadores lineales, entonces
  $$[T \comp S] = [T][S]\,.$$
\end{theo}

\begin{eje}[Base del espacio de operadores en $\C^2$]
  Una base para $\mathcal{L}(\C^2)$ la podemos encontrar a partir de la base canónica del espacio vectorial de las matrices $\mathcal{M}_2(\C)$, que está dada por las matrices:
  \[
    E_{11} = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}, E_{12} = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, E_{21} = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, E_{22} = \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}.
  \]
  Recordando la correspondencia entre matrices y operadores lineales, obtenemos que una base para $\mathcal{L}(\C^2)$ está dada por los operadores $T_{ij}$ definidos por:
  \[
    T_{ij}(v) = v E_{ij}\,.
  \]
  Cualquier operador $T \in \mathcal{L}(\C^2)$ se puede escribir como
  \[
    T = \sum_{i,j=1}^2 t_{ij} T_{ij}\,.
  \]
\end{eje}
