\portada

\begin{esquemaExplorador}
  \temaEsquema{Transformaciones lineales}{
    \conceptoEsquema{Definición y propiedades}{}
    \conceptoEsquema{Núcleo e imagen}{$V/\ker(T) \cong \Ima(T)$}
    \conceptoEsquema{Inyectividad y sobreyectividad}{}
  }
  \temaEsquema{Operadores lineales}{
    \conceptoEsquema{Operadores como endomorfismos}{}
    \conceptoEsquema{Composición de operadores}{}
    \conceptoEsquema{Operador inverso}{}
  }
  \temaEsquema{Representación matricial}{
  \conceptoEsquema{Matriz asociada a una transformación}{$[T]$}
  \conceptoEsquema{cambio de coordenadas}{$[T]_\mathcal{B}^{\mathcal{B}^\prime}$}
  \conceptoEsquema{Isomorfismo con matrices}{}
  }
  \temaEsquema{Matrices complejas}{
    \conceptoEsquema{Operaciones matriciales}{}
    \conceptoEsquema{Matrices especiales}{}
  }
  \temaEsquema{Diagonalización}{
    \conceptoEsquema{Polinomio característico}{$p_A(\lambda) = \det(A-\lambda I)$}
    \conceptoEsquema{Valores y vectores propios}{$vA = \lambda v$}
    \conceptoEsquema{Diagonalización espectral}{$A = PDP^{-1}$}
    \conceptoEsquema{Diagonalización SVD}{$A = U\Sigma V^\dagger$}
  }
\end{esquemaExplorador}

\unirsection{Ideas clave}

\subsection{Introducción y objetivos}

Los operadores lineales constituyen el puente conceptual entre la estructura algebraica abstracta de los espacios vectoriales y su representación computacional mediante matrices. En computación cuántica, este concepto es fundamental ya que todas las operaciones que pueden realizarse sobre sistemas cuánticos se describen mediante operadores lineales unitarios.

La importancia de los operadores lineales en computación cuántica se refleja en múltiples aspectos:

\begin{itemize}
  \item Las \textbf{puertas cuánticas} son operadores unitarios que actúan sobre qubits.
  \item La \textbf{evolución temporal} de sistemas cuánticos se describe mediante operadores unitarios.
  \item Los \textbf{observables cuánticos} son operadores hermitianos.
  \item Los \textbf{algoritmos cuánticos} se construyen como secuencias de operadores lineales.
\end{itemize}

En este tema desarrollaremos la teoría de transformaciones lineales entre espacios vectoriales complejos, con especial énfasis en los operadores (transformaciones de un espacio en sí mismo) y su representación matricial. Esta conexión entre conceptos abstractos y representaciones concretas es esencial para la implementación práctica de algoritmos cuánticos.

\subsection{Transformaciones lineales}

\begin{defi}[Transformación lineal]
  Sean $V$ y $W$ espacios vectoriales. Una función $T: V \to W$ es una \textbf{transformación lineal} o \textbf{homomorfismo}, si para todo $u, v \in V$ y $\alpha, \beta \in \C$ se tiene que
  $$T(\alpha u + \beta v) = \alpha T(u) + \beta T(v)\,.$$
\end{defi}
Una definición equivalente es dar las propiedades por separado.

\begin{prop}
  Una transformación entre espacios vectoriales, $T$ es lineal si y solo si
  \begin{enumerate}
    \item \textbf{Conservación de la suma}. Para todo $u, v \in V$, se tiene que $T(u + v) = T(u) + T(v)$.
    \item \textbf{Conservación del producto por escalar}. Para todo $u \in V$ y $\alpha \in \C$, se tiene que $T(\alpha u) = \alpha T(u)$.
  \end{enumerate}
\end{prop}
\begin{proof}
  La conservación de la suma se cumple de la definición de transformación lineal, al tomar $\alpha = \beta = 1$. Por otro lado, la conservación del producto por escalar se cumple al tomar $\beta = 0$ y $v=0$.

  El recíproco se cumple pues al considerarse $\alpha u$ y $\beta v$ como vectores, aplicamos primero la conservación de la suma y luego la conservación del producto por escalar
  \[
    T(\alpha u + \beta v) = T(\alpha u) + T(\beta v) = \alpha T(u) + \beta T(v)\,.
  \]
\end{proof}

\begin{eje}[Transformaciones lineales básicas]
  \begin{enumerate}
    \item \textbf{Transformación identidad}: $I: V \to V$ definida por $I(v) = v$.

    \item \textbf{Transformación cero}: $O: V \to W$ definida por $O(v) = 0$.

    \item \textbf{Escalamiento}: Dado $\alpha \in \C$, $S_\alpha: \C^n \to \C^n$ definida por $S_\alpha(v) = \alpha v$.

    \item \textbf{Proyección}: $P: \C^3 \to \C^2$ definida por $P(x, y, z) = (x, y)$.
  \end{enumerate}
\end{eje}

Aunque es sencillo de comprobar, es intersante resaltar que toda aplicación lineal debe conservar el elemento nulo y el elemento opuesto.
\begin{align*}
  T(0)  & = T(0+0) = T(0) + T(0)\Rightarrow T(0) = 0\,. \\
  T(-v) & = T(-1\cdot v) = -T(v)\,.
\end{align*}
\begin{prop}
  \label{prop:propiedadesTL}
  Si $T$ y $U$ son transformaciones lineales entre espacios vectoriales, y existe $\mathcal{B} = \{v_1, \ldots, v_n\}$ una base de $V$ tal que $U(v_i) = T(v_i)$ para todo $i$, entonces $U = T$.
\end{prop}
\begin{proof}
  Si tenemos un vector $v=\sum_{i=1}^n \alpha_i v_i$, entonces
  \begin{align*}
    U(v) & = U\left(\sum_{i=1}^n \alpha_i v_i\right) = \sum_{i=1}^n \alpha_i U(v_i) = \sum_{i=1}^n \alpha_i T(v_i) = T\left(\sum_{i=1}^n \alpha_i v_i\right) = T(v)\,.
  \end{align*}
\end{proof}

\begin{defi}
  Sea $T: V \to W$ una transformación lineal. Diremos que $T$ es:
  \begin{itemize}
    \item \textbf{Monomorfismo} si es inyectiva.
    \item \textbf{Epimorfismo} si es sobreyectiva.
    \item \textbf{Isomorfismo} si es biyectiva. En este caso también se dice que los espacios vectoriales son isomorfos y se escribe como $V \cong W$.
  \end{itemize}
\end{defi}

\begin{defi}[Núcleo]
  Sea $T: V \to W$ una transformación lineal. Llamamos \textbf{núcleo (o kernel)} de $T$ a
  \[
    \Ker(T) = \{v \in V : T(v) = 0\}
  \]
\end{defi}

\begin{prop}
  \label{prop:propiedadesKerIma}
  Sea $T: V \to W$ una transformación lineal. Entonces:
  \begin{enumerate}
    \item $\Ker(T)$ es un subespacio de $V$.
    \item $\Ima(T)$ es un subespacio de $W$.
    \item $T$ es inyectiva si y solo si $\Ker(T) = \{0\}$.
  \end{enumerate}
\end{prop}
\begin{proof}
  Se deja al lector como ejercicio (~\ref{ex:propiedadesKerIma}).
\end{proof}

\begin{eje}[Cálculo de núcleo e imagen]
  Considerar la transformación $T: \C^3 \to \C^2$ definida por:
  \[
    T\mqty(x\\ y\\ z) = \mqty(x + iy\\ 2x - z)\,.
  \]
  \textbf{Núcleo:} Para obtener el núcleo de la transformación lineal, tenemos que resolver la ecuación obtenida al plantear la igualdad $T(v) = 0$.
  \begin{align}
    x + iy & = 0 \\
    2x - z & = 0
  \end{align}
  De la primera ecuación: $x = -iy$. De la segunda: $z = 2x = -2iy$. Por tanto
  $$
    \Ker(T) = \left\{\mqty(-iy\\ y\\ -2iy)\mid y \in \C\right\} = \text{gen}\left\{\mqty(-i\\ 1\\ -2i)\right\}\,.
  $$

  \textbf{Imagen:} Como $T$ es lineal, $\Ima(T) = \text{gen}\{T(e_1), T(e_2), T(e_3)\}$, donde $\{e_1, e_2, e_3\}$ es la base canónica de $\C^3$.
  \begin{align}
    T(e_1) & = \mqty(1 \\ 2), \\
    T(e_2) & = \mqty(i \\ 0), \\
    T(e_3) & = \mqty(0 \\ -1)\,.
  \end{align}
  Como $\mqty(1\\ 2) = -i\mqty(i\\ 0) - 2\mqty(0\\ -1)$ y los vectores $\mqty(i\\ 0)$ y $\mqty(0\\ -1)$ son linealmente independientes en $\C^2$
  \[
    \Ima(T) = \text{gen}\left\{\mqty(i\\ 0), \mqty(0\\ -1)\right\} = \C^2\,.
  \]
\end{eje}

Como vemos, un homomorfismo genera un subespacio que puede ser menor al espacio de llegada, y nos interesa saber cuanto nos falta para llenar dicho espacio.

\begin{theo}
  \label{th:descomposicion}
  Sea $T: V \to W$ una transformación lineal. Entonces:
  \begin{itemize}
    \item Existe $V^\prime \subseteq V$ tal que $V = \Ker(T) \oplus V^\prime$.
    \item Existe $W^\prime \subseteq W$ tal que $W = \Ima(T) \oplus W^\prime$.
    \item $V/\Ker(T) \cong \Ima(T)$.
  \end{itemize}
\end{theo}

Como consecuencia directa del Teorema~\ref{th:descomposicion} obtenemos el siguiente resultado fundamental.

\begin{theo}
  \label{th:dimension}
  Sea $T: V \to W$ una transformación lineal. Entonces
  \[
    \dim(V) = \dim(\Ker(T)) + \dim(\Ima(T))\,.
  \]
\end{theo}

Es por este resultado que es importante estudiar el núcleo de un homomorfismo.

\subsection{Operadores lineales}

\begin{defi}[Operador lineal]
  Un \textbf{operador lineal} o \textbf{endomorfismo} sobre un espacio vectorial $V$ es una transformación lineal $T: V \to V$. El conjunto de todos los operadores lineales de $V$ se denota $\mathcal{L}(V)$.
\end{defi}

Los operadores lineales tienen propiedades especiales debido a que el espacio de salida coincide con el de entrada, lo que permite usar conceptos como la composición o la inversa.

Habitualmente se omite la parte de lineal y se habla solo de operadores, pero no debemos nunca olvidar esta condición.

\begin{prop}
  \label{prop:propiedadesOperadores}
  Sean $R,S$ y $T$ operadores lineales. La composición de operadores lineales satisface:
  \begin{enumerate}
    \item \textbf{Asociativa:} $(R \comp S) \comp T = R \comp (S \comp T)$.
    \item \textbf{Elemento neutro:} $I \comp T = T \comp I = T$.
    \item \textbf{Distributiva:} $R \comp (S + T) = R \comp S + R \comp T$.
  \end{enumerate}
\end{prop}
\begin{proof}
  Se deja al lector como ejercicio (~\ref{ex:propiedadesOperadores}).
\end{proof}

\begin{theo}[Caracterización de la invertibilidad]
  Un operador lineal $T \in \mathcal{L}(V)$ es invertible si y solo si es un isomorfismo. Equivalentemente:
  \begin{enumerate}
    \item $T$ es inyectivo ($\Ker(T) = \{0\}$).
    \item $T$ es sobreyectivo ($\Ima(T) = V$).
  \end{enumerate}
\end{theo}

Aplicando el Teorema~\ref{th:descomposicion} a operadores lineales obtenemos la siguiente consecuencia.
\begin{theo}
  Sea $T \in \mathcal{L}(V)$ un operador lineal, se cumple
  \[
    V = \Ker(T) \oplus \Ima(T)\,.
  \]
\end{theo}

\subsection{Representación matricial de transformaciones lineales}

La representación matricial establece una correspondencia biyectiva entre transformaciones lineales y matrices, permitiendo cálculos computacionales eficientes.

\begin{defi}[Matriz de una transformación lineal]
  Sea $T: V \to W$ una transformación lineal. Sean $\mathcal{B}_V = \{v_1, \ldots, v_n\}$ y $\mathcal{B}_W = \{w_1, \ldots, w_m\}$ bases de $V$ y $W$ respectivamente.

  Para cada $j = 1, \ldots, n$, expresamos $T(v_j)$ en la base $\mathcal{B}_W$
  \[
    T(v_j) = \sum_{k=1}^m a_{jk} w_k\,.
  \]

  La \textbf{matriz de} $T$ \textbf{respecto a las bases} $\mathcal{B}_V$ \textbf{y} $\mathcal{B}_W$ es
  \[
    [T]_{\mathcal{B}_V}^{\mathcal{B}_W} = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1m} \\ a_{21} & a_{22} & \cdots & a_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nm} \end{pmatrix}\,.
  \]
\end{defi}
\separa
Cuando al escrbir la matriz, o las coordenadas de un vector, no se especifica la base, se considera la base canónica.
En este caso, la matriz de la transformación se denota simplemente como $[T]$.

\begin{nota}
  La $j$-ésima fila de $[T]_{\mathcal{B}_V}^{\mathcal{B}_W}$ contiene las coordenadas de $T(v_j)$ respecto a la base $\mathcal{B}_W$
  \setlength{\belowdisplayskip}{0pt}
  \begin{equation*}
    T(v_j)_{\mathcal{B}_W} = (v_j)_{\mathcal{B}_V} [T]_{\mathcal{B}_V}^{\mathcal{B}_W}\,.
  \end{equation*}
\end{nota}

\begin{eje}[Matriz de una transformación]
  Sea $T: \C^2 \to \C^3$ definida por $T\mqty(x\\ y) = \mqty(x + iy\\ 2x\\ x - y)$.

  Usando las bases canónicas:
  \begin{align*}
    T(e_1) & = \mqty(1 \\ 2\\ 1)     \\
    T(e_2) & = \mqty(i \\ 0\\ -1)\,.
  \end{align*}
  Por tanto
  \begin{equation*}
    [T] = \begin{pmatrix} 1 & 2 & 1 \\ i & 0 & -1 \end{pmatrix}
  \end{equation*}
\end{eje}

\begin{theo}[cambio de coordenadas]
  \label{th:cambio_base}
  Sea $T\in\mathcal{L}(V)$ un operador lineal y $\mathcal{B}_1$, $\mathcal{B}_2$, $\mathcal{B}_3$ y $\mathcal{B}_4$ bases de $V$. Existe una matriz $P$ invertible tal que las matrices $[T]_{\mathcal{B}_1}^{\mathcal{B}_3}$ y $[T]_{\mathcal{B}_2}^{\mathcal{B}_4}$ están relacionadas por
  $$[T]_{\mathcal{B}_2}^{\mathcal{B}_4} = P[T]_{\mathcal{B}_1}^{\mathcal{B}_3}P^{-1}\,.$$
\end{theo}

Este teorema es muy importante, pues nos permite restringir el estudio de muchas propiedades en operadores lineales, con solo comprobarlo con la matriz asociada a las bases canónicas.

\subsection{Matrices complejas como operadores}

Hemos visto, que todo operador lineal $T\in\mathcal{L}(V)$ puede representarse como una matriz cuadrada $A\in\C^{n\times n}$, donde $n=\dim(V)$. Pero el recíproco es claramente cierto, es decir, todo matriz cuadrada define un operador lineal.

Cada matriz compleja $A \in \mathcal{M}_n(\C)$ define naturalmente un operador lineal $T_A: \C^n \to \C^n$ mediante $T_A(v) = vA$. Esta correspondencia permite estudiar propiedades algebraicas de las matrices a través de la teoría de operadores, y viceversa.

\begin{defi}
  Sea $A\in \mathcal{M}_n(\C)$. Llamamos \textbf{matriz adjunta} a la matriz $A^\dagger$ definida por
  \[
    (A^\dagger)_{ij} = \conj{A_{ji}}\,.
  \]
\end{defi}

\begin{defi}
  Sea $A \in \mathcal{M}_n(\C)$. Decimos que $A$ es:
  \begin{itemize}
    \item
          \textbf{Hermítica}: si $A^\dagger = A$.
    \item
          \textbf{Unitaria}: si $A^\dagger A = AA^\dagger = I$.
    \item
          \textbf{Normal}: si $A^\dagger A = AA^\dagger$.
  \end{itemize}

\end{defi}

Estudiaremos con más detalle estas matrices en temas posteriores, ya que son fundamentales en computación cuántica.

\begin{eje}[Matrices de Pauli]
  Las matrices de Pauli son matrices hermitianas y unitarias fundamentales en computación cuántica:
  \begin{align}
    \sigma_x = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}, \quad
    \sigma_y = \begin{pmatrix} 0 & -i \\ i & 0 \end{pmatrix}, \quad
    \sigma_z = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}
  \end{align}

  Estas matrices satisfacen:
  \begin{itemize}
    \item $\sigma_i^2 = I$ para $i = x, y, z$.
    \item $\sigma_x\sigma_y = i\sigma_z$, $\sigma_y\sigma_z = i\sigma_x$, $\sigma_z\sigma_x = i\sigma_y$.
    \item $\{\sigma_i, \sigma_j\} = \sigma_i\sigma_j + \sigma_j\sigma_i = 0$ para $i \neq j$ (anticonmutan).
    \item Junto con la identidad $I$, forman una base de $\mathcal{M}_2(\C)$.
  \end{itemize}
\end{eje}

\begin{eje}[Matriz de Hadamard]
  La matriz de Hadamard es otra de las matrices unitarias fundamentales
  \[
    H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}
  \]
  Se verifica que $H^2 = I$ y transforma la base canónica en:
  \begin{align*}
    H(1, 0) & = \frac{1}{\sqrt{2}}(1, 1)\,.  \\
    H(0, 1) & = \frac{1}{\sqrt{2}}(1, -1)\,.
  \end{align*}
\end{eje}

Es importante recordar que las propiedades de hermitianidad, unitariedad y normalidad no dependen de las bases escogidas.

\subsection{Valores y vectores propios}

\begin{defi}[Valor y vector propio]
  Sea $A \in \mathcal{M}_n(\C)$. Un escalar $\lambda \in \C$ es un \textbf{valor propio} de $A$ si existe un vector no nulo $v \in \C^n$ tal que
  $Av = \lambda v$.

  El vector $v$ se llama \textbf{vector propio} asociado al valor propio $\lambda$.
\end{defi}

Para obtener los valores propios de una matriz $A$, se resuelve el sistema $(A - \lambda I)v = 0$. Este sistema tiene soluciones no triviales si y solo si $\det(A - \lambda I) = 0$.

\begin{defi}[Polinomio característico]
  El \textbf{polinomio característico} de $A \in \mathcal{M}_n(\C)$ es
  \[
    p_A(\lambda) = \det(A - \lambda I)\,.
  \]
\end{defi}

\begin{prop}
  Los valores propios de $A\in\mathcal{M}_n(\C)$ son las raíces del polinomio característico $p_A(\lambda)$.
\end{prop}

\begin{eje}
  Consideremos la matriz de Pauli $\sigma_z = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$, cuyo polinomio característico es

  \[
    p_{\sigma_z}(\lambda) = \det\begin{pmatrix} 1-\lambda & 0 \\ 0 & -1-\lambda \end{pmatrix} = (1-\lambda)(-1-\lambda) = \lambda^2 - 1\,.
  \]

  Los valores propios son $\lambda_1 = 1$ y $\lambda_2 = -1$.

  Ahora calculemos los vectores propios, primero para $\lambda_1 = 1$
  \[
    (\sigma_z - I)v = \begin{pmatrix} 0 & 0 \\ 0 & -2 \end{pmatrix}\mqty(v_1\\ v_2) = 0 \Rightarrow v_2 = 0\,.
  \]
  Por tanto, $\mqty(v_1 \\ 0)$ con $v_1 \neq 0$ es un vector propio de $\sigma_z$ asociado al valor propio $\lambda_1 = 1$.

  Los vectores propios para $\lambda_2 = -1$ son
  \[
    (\sigma_z + I)v = \begin{pmatrix} 2 & 0 \\ 0 & 0 \end{pmatrix}\mqty(v_1\\ v_2) = 0 \Rightarrow v_1 = 0\,.
  \]
  Por tanto, $\mqty(0\\ v_2)$ con $v_2 \neq 0$ es un vector propio de $\sigma_z$ asociado al valor propio $\lambda_2 = -1$.
\end{eje}

\begin{theo}[Diagonalización]
  \label{th:diagonalizacion}
  Una matriz $A \in \mathcal{M}_n(\C)$ es diagonalizable si y solo si los vectores propios asociados a los valores propios forman una base de $\C^n$. En tal caso, existe una matriz invertible $P$ tal que
  \[
    A = P^{-1}DP\,,
  \]
  donde $D$ es diagonal con los valores propios de $A$ en la diagonal y $P$ es la matriz cuyas fila $i$-ésiam es el vector propio del valor propio $i$-ésimo de $A$.
\end{theo}

\begin{defi}[Diagonalización espectral]
  Sea $A \in \mathcal{M}_n(\C)$ una matriz diagonalizable. Diremos que $A$ es \textbf{diagonalizable espectral} cuando tiene $n$ valores propios distintos. Llamaremos \textbf{espectro} de $A$ al conjunto de sus valores propios ordenados de mayor a menor.
\end{defi}

\begin{eje}
  Consideremos la matriz de Hadamard $H = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$.
  Si calculamos su polinomio característico tenemos
  \[
    p_H(\lambda) = \det(H - \lambda I) = \begin{vmatrix} \frac{1}{\sqrt{2}}-\lambda & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}-\lambda \end{vmatrix} = (\frac{1}{\sqrt{2}}-\lambda)(-\frac{1}{\sqrt{2}}-\lambda) - \frac{1}{2} =  \lambda^2 - 1\,.
  \]

  Los valores propios son $\lambda_1 = 1$ y $\lambda_2 = -1$.

  Para $\lambda_1 = 1$ tenemos
  \[
    (H - I)v = \begin{pmatrix} \frac{1-\sqrt{2}}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{-1-\sqrt{2}}{\sqrt{2}} \end{pmatrix}\mqty(v_1\\v_2) = 0 \Rightarrow \begin{cases}
      v_1(1-\sqrt{2}) + v_2 = 0 \\
      v_1 - v_2(1+\sqrt{2}) = 0\,.
    \end{cases}
  \]
  Por tanto, $\mqty(1+\sqrt{2}\\ 1)$ es un vector propio de $H$ asociado al valor propio $\lambda_1 = 1$.

  Para $\lambda_2 = -1$ tenemos
  \[
    (H + I)v =\begin{pmatrix} \frac{1+\sqrt{2}}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{-1+\sqrt{2}}{\sqrt{2}} \end{pmatrix}\mqty(v_1\\ v_2) = 0 \Rightarrow \begin{cases}
      v_1(1+\sqrt{2}) + v_2 = 0 \\
      v_1 + v_2(-1+\sqrt{2}) = 0\,.
    \end{cases}
  \]
  Por tanto, $\mqty(1-\sqrt{2}\\ 1)$ es un vector propio de $H$ asociado al valor propio $\lambda_2 = -1$.

  Por el teorema de la diagonalización tenemos que
  \[
    H = \frac{1}{2\sqrt{2}}\begin{pmatrix} 1 & -1 \\ -1+\sqrt{2} & 1+\sqrt{2} \end{pmatrix}\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}\begin{pmatrix} 1+\sqrt{2} & 1 \\ 1-\sqrt{2} & 1 \end{pmatrix}\,.
  \]
\end{eje}

\subsection{Descomposición en valores singulares}

Una característica importante de las matrices complejas, es su descomposición en valores singulares, también llamada SVD (Singular Value Decomposition) por sus siglas en inglés, que permite representar una matriz como una combinación de matrices unitarias y una matriz diagonal.

\begin{prop}
  Sea $A \in \mathcal{M}_n(\C)$. Existe una descomposición $A = UDV^\dagger$, donde $U$ y $V$ son matrices unitarias y $D$ es una matriz diagonal con valores reales no negativos y ordenados de mayor a menor.
\end{prop}

\begin{defi}
  A la matriz diagonal resultado de la descomposición en valores singulares la llamamos \textbf{matriz singular} y los elementos de la diagonal los llamamos \textbf{valores singulares} de $A$.
\end{defi}

Habitualmente, la descomposición en valores singulares tiene su propia notación y se denota por $\Sigma$ a la matriz diagonal y por $\sigma_i$ a los valores singulares de $A$.

\begin{eje}
  Consideremos la matriz $A = \frac{1}{2}\begin{pmatrix} 4 & 0 \\ \sqrt{3}i & \sqrt{5} \end{pmatrix}$.

  Para calcular la descomposición en valores singulares de $A$ debemos calcular los valores propios de la matrices $AA^\dagger$ y $A^\dagger A$.
  \begin{align*}
    AA^\dagger  & = \frac{1}{4}\begin{pmatrix} 4 & 0 \\ \sqrt{3}i & \sqrt{5} \end{pmatrix}\begin{pmatrix} 4 & -\sqrt{3}i\\ 0 & \sqrt{5} \end{pmatrix} = \mqty(4             & -\sqrt{3}i  \\ \sqrt{3}i & 2)\,.               \\
    A^\dagger A & = \frac{1}{4}\begin{pmatrix} 4 & -\sqrt{3}i\\ 0 & \sqrt{5} \end{pmatrix}\begin{pmatrix} 4 & 0 \\ \sqrt{3}i & \sqrt{5} \end{pmatrix} = \frac{1}{4}\mqty(19 & -\sqrt{15}i \\ \sqrt{15}i & 5)\,.
  \end{align*}

  El polinomio característico de $AA^\dagger$ y $A^\dagger A$ es
  \begin{align*}
    p_{AA^\dagger}(\lambda)  & = \mqty|4-\lambda              & -\sqrt{3}i  \\ \sqrt{3}i & 2-\lambda| \\
                             & = \lambda^2-6\lambda+5\,.                    \\
    p_{A^\dagger A}(\lambda) & = \frac{1}{16}\mqty(19-\lambda & -\sqrt{15}i \\ \sqrt{15}i & 5-\lambda)\\
                             & = \lambda^2-6\lambda+5\,.
  \end{align*}

  Como esparábamos, ambas matrices tienen el mismo polinomio característico, y por tanto los mismos valores propios, que son
  \[
    \lambda =\frac{6\pm\sqrt{36-20}}{2} = \frac{6\pm\sqrt{16}}{2} = \{5, 1\}\,.
  \]

  Como los valores singulares de $A$ son las raices cuadradas de los valores propios, obtenemos los valores singulares
  \[
    \sigma_1 = \sqrt{5} \quad \text{y}\quad \sigma_2 = 1\,.
  \]

  Al ordenar de mayor a menor obtenemos la matriz singular
  \[
    \Sigma = \begin{pmatrix} \sqrt{5} & 0 \\ 0 & 1 \end{pmatrix}\,.
  \]

  Ahora tenemos que calcular los vectores singulares normalizados de $AA^\dagger$ y $A^\dagger A$:
  \begin{itemize}
    \item Para $\sigma_1 = 5$ un vector propio normalizado de $AA^\dagger$ es $u_1 = \frac{1}{2}\mqty(-\sqrt{3} \\ -i)$.
    \item Para $\sigma_1 = 1$ un vector propio normalizado es $AA^\dagger$ es $u_2 = \frac{1}{2}\mqty(1 \\ -\sqrt{3}i)$.
    \item Para $\sigma_1 = 5$ un vector propio normalizado de $A^\dagger A$ es $v_1 =\frac{1}{4} \mqty(-\sqrt{15} \\ -i)$.
    \item Para $\sigma_1 = 1$ un vector propio normalizado es $A^\dagger A$ es $v_2 =\frac{1}{4} \mqty(1 \\ -\sqrt{15}i)$.
  \end{itemize}

  Por lo tanto las matrices $U$ y $V$ que se construyen con estos vectores son
  \[
    U = \frac{1}{2}\mqty(-\sqrt{3} & 1 \\ -i & -\sqrt{3}i)\quad V = \frac{1}{4}\mqty(-\sqrt{15} & 1 \\ -i & -\sqrt{15}i)
  \]

  Por lo tanto la matriz $A$ puede descomponerse en
  \[
    A = U\Sigma V^\dagger = \frac{1}{8}\mqty(-\sqrt{3} & 1 \\ -i & -\sqrt{3}i)\begin{pmatrix} \sqrt{5} & 0 \\ 0 & 1 \end{pmatrix}\mqty(-\sqrt{15} & -i \\ 1 & -\sqrt{15}i)
  \]
\end{eje}

\subsection{Espacios de operadores}

El estudio de los operadores lineales no se limita a sus propiedades individuales. El conjunto de todos los operadores lineales definidos sobre un espacio vectorial forma, a su vez, un nuevo espacio vectorial. Esta estructura permite sumar operadores y multiplicarlos por escalares, extendiendo las herramientas del álgebra lineal al propio conjunto de transformaciones. En el contexto de la computación cuántica, esta perspectiva es esencial para comprender la estructura del espacio de observables y las operaciones permitidas sobre ellos.

\begin{prop}
  El conjunto $\mathcal{L}(V)$ de todos los operadores lineales en un espacio vectorial $V$ forma un espacio vectorial con las operaciones:
  \begin{itemize}
    \item $(S + T)(v) = S(v) + T(v)$
    \item $(\alpha T)(v) = \alpha T(v)$
  \end{itemize}
\end{prop}

\begin{theo}[Correspondencia entre operadores lineales y matrices]
  La representación matricial establece un isomorfismo de espacios vectoriales
  $$\mathcal{L}(V) \cong \C^{n^2}\cong \mathcal{M}_n(\C)\,,$$
  donde $\dim(V) = n$.

  Además, si $S, T\in\mathcal{L}(V)$ son operadores lineales, entonces
  $$[T \comp S] = [T][S]\,.$$
\end{theo}

\begin{eje}[Base del espacio de operadores en $\C^2$]
  Una base para $\mathcal{L}(\C^2)$ la podemos encontrar a partir de la base canónica del espacio vectorial de las matrices $\mathcal{M}_2(\C)$, que está dada por las matrices:
  \[
    E_{11} = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}, E_{12} = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, E_{21} = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, E_{22} = \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}.
  \]
  Recordando la correspondencia entre matrices y operadores lineales, obtenemos que una base para $\mathcal{L}(\C^2)$ está dada por los operadores $T_{ij}$ definidos por:
  \[
    T_{ij}(v) = v E_{ij}\,.
  \]
  Cualquier operador $T \in \mathcal{L}(\C^2)$ se puede escribir como
  \[
    T = \sum_{i,j=1}^2 t_{ij} T_{ij}\,.
  \]
\end{eje}
